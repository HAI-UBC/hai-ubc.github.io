<!DOCTYPE html>
<html lang="en">

<head>
    <title>Research HAI</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <!-- All Meta -->
    <meta http-equiv="x-ua-compatible" content="ie=edge" />
    <meta name="description"
        content="HAI UBC is a research group led by Dr. Cristina Conati in the Department of Computer Science at The University of British Columbia." />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
    <link rel="shortcut icon" href="assets/images/_logohai.png" type="image/png">
    <!--style css-->
    <link rel="stylesheet" href="assets/css/style.css">
</head>
<div class="whole">

    <body style="background-color: white;">

        <div class="topnav">
            <div class="logo">
                <a href=index.html><b>Human-AI Interaction @ UBC</b></a>
            </div>
            <div class="buttons">
                <a href="index.html">Home</a>
                <a href="people.html">People</a>
                <a class="active" href="research.html">Research</a>
                <a href="publication.html">Publication</a>

            </div>
        </div>

        <br>
        <div class="containerresearch">

            <!-- XAI -->
            <h2>Personalized Explainable AI</h2>

            <p align="justify"> Existing research on Explainable AI (XAI) suggests that having AI systems explain their
                inner workings to their end users can help foster transparency, interpretability, and trust. However,
                results also suggest that such explanations are not always wanted by or beneficial for all users. This
                project aims to understand when it is useful to enable AI systems to provide explanations that justify
                their behavior, and how this may depend on factors such as context, task criticality, and user
                differences (e.g., expertise, personality, cognitive abilities, and transient states like confusion or
                cognitive load). We already have evidence that user personality traits such as need for cognition, and
                openness impact how users interact with and benefit from explanations provided by a music recommender
                system and an ITS, supporting the vision of a personalized XAI endowing AI agents with the ability to
                understand to whom, when, and how to provide explanations. </p>



            <div class="projects">

                <div class="project">
                    <div class="card">
                        <h4>Toward personalized XAI for Intelligent Tutoring System</h4>
                        <img src="assets/images/xai_its2.png" alt="Project 2 image">
                        <p align="Center"><a href="publication.html?q=XAI%20ITS">
                                Publications</a></p>
                    </div>
                </div>

                <div class="project">
                    <div class="card">
                        <h4>Personalized explanations for a music recommender system</h4>
                        <img src="assets/images/xai_music.png" alt="Project 1 image">

                        <p align="Center"><a href="publication.html?q=Music%20Recommender">
                                Publications</a></p>
                    </div>

                </div>


            </div>


            <hr>

            <!-- Eye Tracking -->
            <h2>Using Eye-Tracking Data in User Modeling</h2>

            <p align="justify"> Off-line analysis of eye-tracking data has been commonly used to evaluate user
                interfaces in HCI and to understand cognitive and perceptual processes in Psychology. We investigate how
                to leverage eye-tracking data to train machine learning (ML) models that an AI-driven interactive system
                can utilize to predict in real-time specific traits and states of its users during interaction (user
                modeling), and personalized its responses and behaviors accordingly.
                Our results so far have shown that eye-tracking data can be used for real-time prediction of user traits
                and states relevant for AI-driven personalization in a variety of context, including: user perceptual
                abilities and skills relevant for devising the user-adaptive visualizations; user's learning with
                Intelligent Tutoring Systems, early onset of Alzheimer, user affective states such as individual
                emotions, emotion valence, and confusion.
            </p>



            <div class="projects">
                <div class="project">
                    <div class="card">
                        <h4>Prediction of user properties for user-adaptive visualizations</h4>
                        <img src="assets/images/et_cognitive.png" alt="Project 1 image">


                        <p align="Center"><a href="publication.html?q=User%20properties">
                                Publications</a></p>
                    </div>
                </div>

                <div class="project">
                    <div class="card">
                        <h4> Prediction of users learning with Intelligent Tutoring System </h4>
                        <img src="assets/images/et_its2.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=ET%20ITS">
                                Publications</a></p>
                    </div>
                </div>


                <div class="project">
                    <div class="card">
                        <h4> Detection of Alzheimers disease using eye-tracking and language</h4>
                        <img src="assets/images/et_alz2.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=Alzheimer">
                                Publications</a></p>
                        <p align="Center"><a href="https://canarycognition.med.ubc.ca/">
                                More Info</a></p>
                    </div>
                </div>


                <div class="project">
                    <div class="card">
                        <h4> Detection of user-affective states using eye-tracking </h4>
                        <img src="assets/images/et_useraffective.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=User%20affect">
                                Publications</a></p>
                    </div>
                </div>

            </div>

            <hr>

            <!-- User-Adaptive Visualization -->
            <h2>User-Adaptive Information Visualization</h2>

            <p align="justify"> Information visualization is becoming increasingly important with the explosion of
                applications that allow users to access a large amount of data in many aspects of their lives. Most
                research in information visualization seeks to devise optimal visualizations given the type of data and
                tasks to be supported. There is, however, preliminary evidence in perceptual psychology that
                visualization effectiveness may depend on users' specific characteristics (e.g., preferences, abilities,
                and mental states), calling for the need of user-adaptive information visualization. Our results show
                that cognitive abilities such as perceptual speed, visual and verbal working memory can impact user
                experience with well-known bar-graph-based visualization, more complex visualizations and visualizations
                embedded in narrative text. Our results also provided insights on how personalization should be
                provided. We have shown that providing real-rime support to guide user attention during visualization
                processing can improve user performance, the first result of this nature in the literature. </p>



            <div class="projects">
                <div class="project">
                    <div class="card">
                        <h4>Impact of congnitive abilites on user experience with visualizations </h4>
                        <img src="assets/images/visual.png" alt="Project 1 image">

                        <p align="Center"><a href="publication.html?q=Cognitive%20abilities">
                                Publications</a></p>
                    </div>
                </div>

                <div class="project">
                    <div class="card">
                        <h4>Personalization for user adaptive visualization</h4>
                        <img src="assets/images/viz_eval.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=User%20adaptive">
                                Publications</a></p>
                    </div>
                </div>




            </div>


            <hr>


            <!-- AI in ELE -->
            <h2>AI-driven Personalization in Exploratory Learning Environments</h2>

            <p align="justify"> There is extensive evidence that AI-based educational technology can effectively provide
                personalized support to help students learn problem-solving skills in various domains. In recent years
                there has also been increasing interest in AI-based environments to support educational activities that
                are more exploratory in nature. The capability to explore effectively is relevant to many tasks
                involving interactive systems (e.g., learning from MOOCS, working with an interactive simulation,
                exploring data with tools for visual analytics, and researching a topic online). However, not all
                learners possess the meta-cognitive skills (e.g., reasoning and study processes) to learn effectively
                from these environments. This project investigates AI-based tools that can provide personalized support
                to users who are having difficulties with exploratory learning environments, including interactive
                simulations, an environment to support the acquisition of computational thinking skills via game design
                and MOOCs.
            </p>



            <div class="projects">
                <div class="project">
                    <div class="card">
                        <h4> Personalized support to learning from interactive simulations </h4>
                        <img src="assets/images/ele_sim.png" alt="Project 1 image">

                        <p align="Center"><a href="publication.html?q=Interactive%20Simulation">
                                Publications</a></p>
                    </div>
                </div>

                <div class="project">
                    <div class="card">
                        <h4>Environment to support computational thinking skills via game design
                        </h4>
                        <img src="assets/images/unityct.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=CT">
                                Publications</a></p>
                    </div>
                </div>

                <div class="project">
                    <div class="card">
                        <h4>Providing Adaptive Support for MOOCs </h4>
                        <img src="assets/images/mooc.png" alt="Project 2 image">

                        <p align="Center"><a href="publication.html?q=MOOC">
                                Publications</a></p>
                    </div>
                </div>

            </div>

        </div>

        <footer contentEditable>
            <hr>
            <p align="Center"> &nbsp; Copyrights reserved @ Human-AI Interaction, <a href="cs.ubc.ca">UBC-CS</a></p>
            <hr>
        </footer>
</div>

</body>
</div>



</html>